
cd ~
tar xvf creditcard_insurance.tar.gz
-------------------------
Data preparation in the source systems
DB to Hadoop Data Ingestion:
-------------------------
Note: (This will be happening automatically in the source systems)
Insert into the mysql database - custmaster data in a table and the credit data into 2 tables of 2 timezones:
-------------------------
create database if not exists custdb; 
use custdb;
drop table if exists credits_pst;
drop table if exists credits_cst; 
drop table if exists custmaster;

create table if not exists credits_pst (id integer,lmt integer,sex integer,edu integer,marital integer,age integer,pay integer,billamt integer,defaulter integer,issuerid1 integer,issuerid2 integer,tz varchar(3));

create table if not exists credits_cst (id integer,lmt integer,sex integer,edu integer,marital integer,age integer,pay integer,billamt integer,defaulter integer,issuerid1 integer,issuerid2 integer,tz varchar(3));

create table if not exists custmaster (id integer,fname varchar(100),lname varchar(100),ageval integer,profession varchar(100));

source /home/hduser/creditcard_insurance/2_2_creditcard_defaulters_pst 		-> this executes the insert queries for credits_pst table
source /home/hduser/creditcard_insurance/2_creditcard_defaulters_cst 		-> this executes the insert queries for credits_cst table
source /home/hduser/creditcard_insurance/custmaster				-> this executes the insert queries for custmaster table

2. Sqoop import the data into hive table insure.credits_pst and insure.credits_cst with options such as hive overwrite, number of mappers 2, where hive table created by sqoop with id as bigint, billamt as float

sqoop import \
--connect jdbc:mysql://localhost:3306/custdb \
--username root \
--password root \
--table credits_pst \
--target-dir /user/hduser/insure/ \
--split-by id \
--m 2 \
--hive-overwrite \
--hive-import \
--create-hive-table \
--fields-terminated-by ',' \
--map-column-hive id=bigint,billamt=float \
--hive-table insure.credits_pst \
--direct

hdfs:
/user/hive/warehouse/insure.db/credits_pst	-> created managed table in hive pointing to this hdfs folder with below 2 split files

-rw-r--r--   1 hduser hadoop     101803 2020-10-17 18:17 /user/hive/warehouse/insure.db/credits_pst/part-m-00000
-rw-r--r--   1 hduser hadoop     101884 2020-10-17 18:17 /user/hive/warehouse/insure.db/credits_pst/part-m-00001

sqoop import \
--connect jdbc:mysql://localhost:3306/custdb \
--username root \
--password root \
--table credits_cst \
--target-dir /user/hduser/insure/ \
--split-by id \
--m 2 \
--hive-overwrite \
--hive-import \
--create-hive-table \
--fields-terminated-by ',' \
--map-column-hive id=bigint,billamt=float \
--hive-table insure.credits_cst \
--direct

hdfs:
/user/hive/warehouse/insure.db/credits_cst	-> created managed table in hive pointing to this hdfs folder with below 2 split files

-rw-r--r--   1 hduser hadoop     128997 2020-10-17 19:06 /user/hive/warehouse/insure.db/credits_cst/part-m-00000
-rw-r--r--   1 hduser hadoop     130156 2020-10-17 19:06 /user/hive/warehouse/insure.db/credits_cst/part-m-00001

ETL & ELT using Hive

3. Filter the cst and pst tables with the billamt > 0 and union all both dataset and load into a single table called insure.cstpstreorder using create table as select (CTAS)

create table cstpstreorder
row format delimited fields terminated by ','
stored as textfile
location '/user/hduser/cstpstunion'
as select * from credits_pst where billamt > 0.0 
UNION ALL select * from credits_cst where billamt > 0.0;

hdfs:
-rw-r--r--   1 hduser supergroup     441263 2020-10-17 20:55 /user/hduser/cstpstunion/000000_0

4. Note: We can merge the step 2 and 3 and make the above 2 sqoop statements as 1 sqoop statement to directly import data into the insure.cstpstreorder by writing –query with billamt>0 filter and union of both credits_pst and credits_cst at the MYSQL level itself finally convert the insure.cstpstreorder into external table. If you have some time try it out.

sqoop import \
--connect jdbc:mysql://localhost:3306/custdb \
--username root \
--password root \
--query "select * from credits_pst where billamt > 0.0 UNION ALL select * from credits_cst where billamt > 0.0 and \$CONDITIONS" \
--target-dir /user/hduser/insureUA/ \
--split-by id \
--m 2 \
--hive-overwrite \
--hive-import \
--create-hive-table \
--fields-terminated-by ',' \
--map-column-hive id=bigint,billamt=float \
--hive-table insure.cstpstreorder1 \
--direct

hdfs:
drwxr-xr-x   - hduser supergroup          0 2020-10-17 21:05 /user/hive/warehouse/insure.db/cstpstreorder1

-rw-r--r--   1 hduser hadoop     397147 2020-10-17 21:05 /user/hive/warehouse/insure.db/cstpstreorder1/part-m-00000
-rw-r--r--   1 hduser hadoop     212382 2020-10-17 21:05 /user/hive/warehouse/insure.db/cstpstreorder1/part-m-00001

alter table cstpstreorder1 SET TBLPROPERTIES('EXTERNAL'='TRUE','external.table.purge'='true')

5. Create another external table insure.cstpstpenality in orc file format (show create table insure.cstpstreorder) to get the ddl of the above table and alter as per the below columns for faster table creation and populate with the below ETL transformations applied in the above table cstpstreorder as given below

CREATE external TABLE `insure.cstpstpenality`(
`id` bigint,
`issuerid1` int,
`issuerid2` int,
`lmt` int,
`newlmt` int,
`sex` int,
`edu` int,
`marital` int,
`pay` int,
`billamt` float,
`newbillamt` float,
`defaulter` int) stored as orc LOCATION
'/user/hduser/cstpstpenality';

a. id,issuerid1,issuerid2,lmt,case defaulter when 1 then lmt-(lmt*0.04) else lmt end as
newlmt ,sex,edu,marital,pay,billamt,case defaulter when 1 then billamt+(billamt*0.02) else
billamt end as newbillamt, defaulter from cstpstreorder1 limit 20;

insert overwrite table cstpstpenality 
select id,issuerid1,issuerid2,lmt,case defaulter when 1 then lmt-(lmt*0.04) else lmt end as
newlmt,sex,edu,marital,pay,billamt,case defaulter when 1 then billamt+(billamt*0.02) else
billamt end as newbillamt, defaulter from cstpstreorder1;

hdfs:
-rw-r--r--   1 hduser hadoop     146707 2020-10-18 10:09 /user/hduser/cstpstpenality/000000_0
-------------------------
Data Provisioning using Hive, Sqoop and DistCp
-------------------------
b. Export and overwrite the above data into /user/hduser/defaultersout/ we will be using
this defaultersout data in a later point of time and /user/hduser/nondefaultersout/
locations with defaulter=1 and defaulter=0 respectively using ‘,’ delimiter. We will be
sending this nondefaultersout data to external systems using Distcp/SFTP.

insert overwrite directory '/user/hduser/defaultersout/' 
row format delimited fields terminated by ','
select * from cstpstpenality where defaulter=1;

hdfs:
-rw-r--r--   1 hduser hadoop     165994 2020-10-18 10:42 /user/hduser/defaultersout/000000_0

insert overwrite directory '/user/hduser/nondefaultersout/' 
row format delimited fields terminated by ','
select * from cstpstpenality where defaulter=0;

hdfs:
-rw-r--r--   1 hduser hadoop     568422 2020-10-18 10:49 /user/hduser/nondefaultersout/000000_0
-------------------------
Data Provisioning to the Consumers using DistCP
-------------------------
Copy the data non defaulters data from one cluster (Prod) to another cluster (Non Prod) to /tmp/promocustomers location in hdfs for analytics purpose.

hadoop distcp hdfs://localhost:54310/user/hduser/nondefaultersout /tmp/promocustomers

hdfs:
-rw-r--r--   1 hduser supergroup     568422 2020-10-18 11:00 /tmp/promocustomers/000000_0

-------------------------
Data preparation in the source systems
-------------------------
File system data source ingestion from Cloud using Linux Shell Script
-------------------------
Execute the below shell script to pull the data from Cloud S3, validate, remove trailer data, move to HDFS and archive the data for backup.

bash sfm_insuredata.sh https://s3.amazonaws.com/in.inceptez.bucket1/insurance_project/insuranceinfo.csv

Ensure data is imported from cloud to hdfs and get the date and take a note of the timestamp in the file

hadoop fs -ls /user/hduser/insurance_clouddata

Found 2 items
-rw-r--r--   1 hduser hadoop          0 2020-10-19 00:44 /user/hduser/insurance_clouddata/_SUCCESS
-rw-r--r--   1 hduser hadoop     414543 2020-10-19 00:44 /user/hduser/insurance_clouddata/creditcard_insurance_2020101900

-------------------------
ETL & ELT using Hive
-------------------------
Create a hive table with header line count as 1 and load the insurance dataset.

hive --service metastore
hive

create database if not exists insure;
use insure;
drop table if exists insurance;

CREATE TABLE insurance (IssuerId1 int,IssuerId2 int,BusinessYear int,StateCode string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan string)
row format delimited fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");

load data inpath '/user/hduser/insurance_clouddata' into table insurance;

Delete the invalid data with null issuerid1 and issuerid2 using insert select query

insert overwrite table insurance
select * from insurance where issuerid1 is not null and issuerid2 is not null;
-------------------------
Create a fixed width hive table to load the fixed width states_fixedwidth data using Regex Serde

drop table if exists insure.state_master;

CREATE EXTERNAL TABLE insure.state_master (statecd STRING, statedesc STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES ("input.regex"= "(.{2})(.{20}).*")
LOCATION '/user/hduser/states';

load data local inpath '/home/hduser/creditcard_insurance/states_fixedwidth' overwrite into table
insure.state_master;
-------------------------
Create a managed table on top of the hive output defaulter’s dataset created above.

use insure;

CREATE TABLE defaulters (id int,IssuerId1 int,IssuerId2 int,lmt int,newlmt double,sex int,edu int,marital
int,pay int,billamt int,newbillamt float,defaulter int)
row format delimited fields terminated by ','
LOCATION '/user/hduser/defaultersout';
-------------------------
Create a final managed table (later convert to external table) in orc with snappy compression and load the above 2 tables joined by applying different functions as given below. This table should not allow duplicates when it is empty or if not using overwrite option.
------------------------- 
use insure;

CREATE TABLE insurancestg (IssuerId int,BusinessYear int,StateCode string,statedesc string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string,id int,lmt int,newlmt int,reduced_lmt int,sex varchar(6),grade varchar(20),marital int,pay
int,billamt int,newbillamt float,penality float,defaulter int)
row format delimited fields terminated by ','
LOCATION '/user/hduser/insurancestg';

insert overwrite table insurancestg select concat(i.IssuerId1,i.IssuerId2) as
issuerid,i.businessyear,i.statecode,s.statedesc as
statedesc,i.sourcename,i.networkname,i.networkurl,i.rownumber,i.marketcoverage,i.dentalonlyplan,
d.id,d.lmt,d.newlmt,d.newlmt-d.lmt as reduced_lmt,case when d.sex=1 then 'male' else 'female' end as
sex ,case when d.edu=1 then 'lower grade' when d.edu=2 then 'lower middle grade' when d.edu=3 then
'middle grade' when d.edu=4 then 'higher grade' when d.edu=5 then 'doctrate grade' end as grade
,d.marital,d.pay,d.billamt,d.newbillamt,d.newbillamt-d.billamt as penality,d.defaulter
from insurance i inner join defaulters d
on (i.IssuerId1=d.IssuerId1 and i.IssuerId2=d.IssuerId2)
inner join state_master s
on (i.statecode=s.statecd);

hdfs:
-rw-r--r--   1 hduser hadoop    4403095 2020-10-19 07:27 /user/hduser/insurancestg/000000_0

dfs -rm -r -f /user/hduser/insuranceorc;

drop table if exists insuranceorc;

CREATE TABLE insuranceorc (IssuerId int,BusinessYear int,StateCode string,statedesc string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string,id int,lmt int,newlmt int,reduced_lmt int,sex varchar(6),grade varchar(20),marital int,pay
int,billamt int,newbillamt float,penality int,defaulter int)
row format delimited fields terminated by ','
stored as orcfile
LOCATION '/user/hduser/insuranceorc'
TBLPROPERTIES ("orc.compress"="SNAPPY","immutable"="true");

Insert into insuranceorc select * from insurancestg where issuerid is not null;
-------------------------
Retry running the above same insert query once again and see what happens??
-------------------------
If you get the below error then drop the above table and recreate and insert.
FAILED: SemanticException [Error 10256]: Inserting into a non-empty immutable table is not allowed
insuranceorc

Convert the above table from managed to external, usually we use the below statement if we can’t
create external table in the initial stage itself for example sqoop import hive table can’t be created as
external initially.

ALTER TABLE insuranceorc SET TBLPROPERTIES('EXTERNAL'='TRUE');
-------------------------
write common table expression queries in hive
-------------------------
with T1 as ( select max(penality) as penalitymale from insuranceorc where sex='male'),
T2 as ( select max(penality) as penalityfemale from insuranceorc where sex='female')
select penalitymale,penalityfemale
from T1 inner join T2
ON 1=1;

-------------------------
Data Governance - Redaction and Masking using Hive and Python
-------------------------

create view in hive to restrict few columns, store queries, and apply some masking on sensitive
columns using either query or by using the mask_insure.py function given in the project document.
-------------------------
drop view if exists middlegradeview;

create view middlegradeview as
select issuerid,businessyear,statedesc,sourcename, sex,grade,marital,newbillamt,defaulter
,translate(translate(translate(translate(translate(networkurl,'a','x'),'b','y'),'c','z'),'s','r'),'.com','.aaa') as
maskednetworkurl
from insuranceorc
where grade='middle grade'
and issuerid is not null;

(or)

mask_insure.py 
                                                                                                                       
#!/usr/bin/env python
import sys
import string
import re
import hashlib
while True:
        line = sys.stdin.readline()
        if not line:
                break
        line = string.strip(line, "\n")
        issuerid,businessyear,statedesc,sourcename, defaulter,networkurl = string.split(line, "\t")
        print "\t".join([issuerid,businessyear,hashlib.sha256(statedesc).hexdigest(),hashlib.md5(sourcename).hexdigest(),defaulter,re.sub('b','z',re.sub('j','y',re.sub('/','-',re.sub('.com','.aaa',networkurl))))])
                                                                 

add FILE /home/hduser/mask_insure.py;

create view middlegradeview as
select transform(issuerid,businessyear,statedesc,sourcename, defaulter ,networkurl) using 'python /home/hduser/mask_insure.py'
as (issuerid,businessyear,statedesc,sourcename, defaulter ,maskednetworkurl)
from insuranceorc
where grade='middle grade'
and issuerid is not null;

select * from middlegradeview;
-------------------------
Export the above view data into hdfs location /user/hduser/defaulterinfo with the pipe ‘|’ delimiter
the following columns
issuerid,businessyear,statedesc,sourcename,maskednetworkurl,sex,grade,marital,newbillamt,defaulter

insert overwrite directory '/user/hduser/defaulterinfo' 
row format delimited fields terminated by '|'
select * from middlegradeview;

hdfs:
Found 1 items
-rw-r--r--   1 hduser hadoop      92900 2020-11-04 13:10 /user/hduser/defaulterinfo/000000_0
-------------------------
Data Provisioning to the Consumers into legacy Databases using Sqoop including Validation
-------------------------
Data export using Sqoop into DB
-------------------------
Export the above data into mysql using sqoop

mysql -u root -p
password: root

use custdb;
drop table if exists middlegrade;

create table middlegrade (issuerid integer,businessyear integer,maskedstatedesc
varchar(200),maskedsourcename varchar(100), defaulter varchar(20),maskednetworkurl varchar(200));

quit;

1. Export the masked data into mysql using sqoop export as per the above table structure.
2. Validate the export is properly happened using --validate option in sqoop

sqoop export --connect jdbc:mysql://localhost/custdb --username root --password root --table middlegrade --export-dir /user/hduser/defaulterinfo --fields-terminated-by '|' -validate;

-------------------------
Complex type ETL using Hive
-------------------------
Create a Complex type table to understand how to group the like issuers in a single row using array, struct and map.
-------------------------
Drop table if exists insuranceorc_collection ;

Create table insuranceorc_collection
row format delimited collection items terminated by ', '
stored as orcfile
location '/user/hduser/insuranceorc_collection/'
as select issuerid,named_struct('marital',marital,'sex',sex,'grade',grade) as
personalinfo,collect_set(networkname) as networkname, collect_set(networkurl) networkurl
from insuranceorc group by issuerid,named_struct('marital',marital,'sex',sex,'grade',grade);

hdfs:
-rw-r--r--   1 hduser supergroup       8171 2020-11-04 16:18 /user/hduser/insuranceorc_collection/000000_0

alter table insuranceorc_collection SET TBLPROPERTIES('EXTERNAL'='TRUE');

-------------------------
Select only the issuerid,grade,second networkname,second networkurl where we have more than 1
networkname and networkurl accessed by the customers.
-------------------------
select issuerid,personalinfo.grade,networkname[1],networkurl[1] from insuranceorc_collection 
where size(networkname)=2 and size(networkurl)=2;

-------------------------
Data movement & migration from DB to HBase using Sqoop
-------------------------
create 'custmaster', 'customer'

quit

1. Import using sqoop from db into hbase custmaster data and save the output log into a log file.

sqoop import --connect jdbc:mysql://localhost/custdb --username root --password root --table customer --hbase-table custmaster --hbase-create-table --hbase-bulkload --column-family customer --hbase-row-key custid -m 3 --split-by custid --delete-target-dir -validate &> /tmp/sqoop.log;

2. Validate the sqoop import is properly happened using --validate option in sqoop and create a _SUCCESS file if the validation is successful in /home/hduser/creditcard_insurance location using linux commands.

if['cat /tmp/sqoop.log | grep "Data successfully validated" | wc -l' -eq 1]
then
touch /home/hduser/creditcard_insurance/_HBASE_SUCCESS
fi
-------------------------
Data movement & migration from Hive to HBase using Storage Handler
-------------------------
Create a hbase handler table in hive using hbase storage handler referring to insurancehive table that will be automatically created in hbase with insurance and credit card column families when we create the below hive table.

Use insure;
drop table if exists insurancehive;

CREATE TABLE insurancehive (idkey int, issuerid int,id int,businessyear int,statedesc string,networkurl
string,pay int,defaulter string,billamt int,newbillamt float,penality float)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES
("hbase.columns.mapping" =
":key,insurance:issuerid,insurance:id,insurance:businessyear,insurance:statedesc,insurance:networkurl,
creditcard:pay,creditcard:defaulter,creditcard:billamt,creditcard:newbillamt,creditcard:penality")
TBLPROPERTIES ("hbase.table.name" = "insurancehive",
"hbase.mapred.output.outputtable"="insurancehive");

-------------------------
Insert incremental rowkey as the row_key into HBASE.

insert into table insurancehive select row_number() over() as
idkey,issuerid,id,businessyear,statedesc,networkurl,pay,defaulter,billamt,newbillamt,penality
from insuranceorc where issuerid is not null;
-------------------------
Data Provisioning to the Consumers using Apache Phoenix to support
Realtime aggregation and Low Latency Query processing
-------------------------
Create a phoenix table view on the above hbase table and analyze profession based total payment and average payment.

sqlline.py localhost

!set maxwidth 1000

drop view if exists "insurancehive";

create view "insurancehive" (idkey varchar(100) primary key,"insurance"."issuerid"
varchar,"insurance"."id" varchar,"insurance"."businessyear" varchar,"insurance"."statedesc"
varchar,"insurance"."networkurl" varchar,"creditcard"."pay" varchar,"creditcard"."defaulter"
varchar,"creditcard"."billamt" varchar,"creditcard"."newbillamt" varchar,"creditcard"."penality"
varchar);

drop view if exists "custmaster";

create view "custmaster" (id varchar primary key,"customer"."fname"
varchar,"customer"."lname" varchar,"customer"."profession" varchar,"customer"."ageval" varchar);
-------------------------
Write queries to build cubes at different levels to aggregate the data in realtime to populate in the
report such as average age, sum of bill amount, average bill amount etc.,

select next value for cubeseq as id,lvl, avg_age, sum_billamt, avg_billamt, avg_newbillamt from (
select 1 as lvl,"profession",cast(to_number("ageval") as integer) as avg_age ,0 as sum_billamt,0 as
avg_billamt,0 as avg_newbillamt
from "custmaster"
union all
select 2 as lvl,"profession", cast(to_number("ageval") as integer) as avg_age ,0 as sum_billamt,0 as
avg_billamt,0 as avg_newbillamt
from "custmaster"
union all
select 3 as lvl,"custmaster"."profession",0 as avg_age,cast(to_number("insurancehive"."billamt") as
integer) as sum_billamt,cast(to_number("insurancehive"."newbillamt") as integer) as avg_billamt,
cast(to_number("insurancehive"."newbillamt") as integer) as avg_newbillamt
from "insurancehive" as i inner join "custmaster" as c
on "insurancehive"."id"=c.id
group by "custmaster"."profession") as temp
order by lvl;
-------------------------
select 1 as lvl,"profession",avg(cast(to_number("ageval") as double)) as avg_age ,0.0 as sum_billamt,0.0 as
avg_billamt,0.0 as avg_newbillamt
from "custmaster"
group by lvl,"profession"
union all
select 2 as lvl,"profession", cast(to_number("ageval") as double) as avg_age ,0.0 as sum_billamt,0.0 as
avg_billamt,0.0 as avg_newbillamt
from "custmaster"
union all
select 3 as lvl,"custmaster"."profession",0.0 as avg_age,sum(cast(to_number("insurancehive"."billamt") as
bigint)) as sum_billamt,avg(cast(to_number("insurancehive"."newbillamt") as double)) as avg_billamt,
avg(cast(to_number("insurancehive"."newbillamt") as double)) as avg_newbillamt
from "insurancehive" as i inner join "custmaster" as c
on "insurancehive"."id"=c.id
group by "custmaster"."profession";
-------------------------
-------------------------















